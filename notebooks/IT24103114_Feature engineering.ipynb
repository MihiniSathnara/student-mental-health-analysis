{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04937872-4f1a-4c92-ac0a-362a59b5b9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ce17b33-3645-4aba-803c-9996b12529c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting liac-arff\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: liac-arff\n",
      "  Building wheel for liac-arff (pyproject.toml): started\n",
      "  Building wheel for liac-arff (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11782 sha256=da247eb2fc6f2256015b4309d3713b85546d950aea707eb7a1c519f6e9589b2f\n",
      "  Stored in directory: c:\\users\\param\\appdata\\local\\pip\\cache\\wheels\\93\\f3\\5b\\658a9bddee916a5f4b84bdc1a4e0fabd22fb17947c2c9542e6\n",
      "Successfully built liac-arff\n",
      "Installing collected packages: liac-arff\n",
      "Successfully installed liac-arff-2.5.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install liac-arff"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e21b4dfa-4c85-428b-9814-beab9e1cb7c8",
   "metadata": {},
   "source": [
    "from scipy.io import arff\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fed5db4-c2fb-432c-92e5-ac8c1df6b891",
   "metadata": {},
   "source": [
    "with open(r\"C:\\Users\\param\\Downloads\\depression_dataset.arff\",'r')as f:\n",
    "    dataset = arff.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd165c6f-f40c-4d2d-a60c-1a40ac75fbf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'arff' has no attribute 'loadarff'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Step 1: Load the dataset from ARFF file\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Justification: The dataset is provided in ARFF format, which is common for machine learning datasets.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# We use scipy's arff loader to read it and convert to a Pandas DataFrame for easier manipulation.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data, meta = \u001b[43marff\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadarff\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mparam\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mdepression_dataset.arff\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m df = pd.DataFrame(data)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Decode byte strings if any (ARFF may store strings as bytes)\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'arff' has no attribute 'loadarff'"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset from ARFF file\n",
    "# Justification: The dataset is provided in ARFF format, which is common for machine learning datasets.\n",
    "# We use scipy's arff loader to read it and convert to a Pandas DataFrame for easier manipulation.\n",
    "data, meta = arff.loadarff(\"C:\\\\Users\\\\param\\\\Downloads\\\\depression_dataset.arff\")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Decode byte strings if any (ARFF may store strings as bytes)\n",
    "for col in df.select_dtypes([object]).columns:\n",
    "    df[col] = df[col].str.decode('utf-8')\n",
    "\n",
    "# Display the first few rows to verify loading\n",
    "print(\"Dataset loaded successfully. First 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f21fcf9-fa84-41ce-9a64-00cb2f392194",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "String attributes not supported yet, sorry",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m arff\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data, meta = \u001b[43marff\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mparam\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDownloads\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdepression_dataset.arff\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df = pd.DataFrame(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\io\\arff\\_arffread.py:804\u001b[39m, in \u001b[36mloadarff\u001b[39m\u001b[34m(f)\u001b[39m\n\u001b[32m    802\u001b[39m     ofile = \u001b[38;5;28mopen\u001b[39m(f)\n\u001b[32m    803\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_loadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mofile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ofile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f:  \u001b[38;5;66;03m# only close what we opened\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\io\\arff\\_arffread.py:837\u001b[39m, in \u001b[36m_loadarff\u001b[39m\u001b[34m(ofile)\u001b[39m\n\u001b[32m    826\u001b[39m \u001b[38;5;66;03m# XXX The following code is not great\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;66;03m# Build the type descriptor descr and the list of converters to convert\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;66;03m# each attribute to the suitable type (which should match the one in\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m \u001b[38;5;66;03m# This can be used once we want to support integer as integer values and\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;66;03m# not as numeric anymore (using masked arrays ?).\u001b[39;00m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hasstr:\n\u001b[32m    835\u001b[39m     \u001b[38;5;66;03m# How to support string efficiently ? Ideally, we should know the max\u001b[39;00m\n\u001b[32m    836\u001b[39m     \u001b[38;5;66;03m# size of the string before allocating the numpy array.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mString attributes not supported yet, sorry\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    839\u001b[39m ni = \u001b[38;5;28mlen\u001b[39m(attr)\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerator\u001b[39m(row_iter, delim=\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    842\u001b[39m     \u001b[38;5;66;03m# TODO: this is where we are spending time (~80%). I think things\u001b[39;00m\n\u001b[32m    843\u001b[39m     \u001b[38;5;66;03m# could be made more efficiently:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    854\u001b[39m     \u001b[38;5;66;03m# Note, I have already tried zipping the converters and\u001b[39;00m\n\u001b[32m    855\u001b[39m     \u001b[38;5;66;03m# row elements and got slightly worse performance.\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: String attributes not supported yet, sorry"
     ]
    }
   ],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "data, meta = arff.loadarff(r\"C:\\Users\\param\\Downloads\\depression_dataset.arff\")\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999b777f-e00c-4d81-a7bd-591d2fd37ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id  Gender   Age           City Profession  Academic Pressure  \\\n",
      "0   2.0    Male  33.0  Visakhapatnam    Student                5.0   \n",
      "1   8.0  Female  24.0      Bangalore    Student                2.0   \n",
      "2  26.0    Male  31.0       Srinagar    Student                3.0   \n",
      "3  30.0  Female  28.0       Varanasi    Student                3.0   \n",
      "4  32.0  Female  25.0         Jaipur    Student                4.0   \n",
      "\n",
      "   Work Pressure  CGPA  Study Satisfaction  Job Satisfaction  \\\n",
      "0            0.0  8.97                 2.0               0.0   \n",
      "1            0.0  5.90                 5.0               0.0   \n",
      "2            0.0  7.03                 5.0               0.0   \n",
      "3            0.0  5.59                 2.0               0.0   \n",
      "4            0.0  8.13                 3.0               0.0   \n",
      "\n",
      "      Sleep Duration Dietary Habits   Degree  \\\n",
      "0          5-6 hours        Healthy  B.Pharm   \n",
      "1          5-6 hours       Moderate      BSc   \n",
      "2  Less than 5 hours        Healthy       BA   \n",
      "3          7-8 hours       Moderate      BCA   \n",
      "4          5-6 hours       Moderate   M.Tech   \n",
      "\n",
      "  Have you ever had suicidal thoughts ?  Work/Study Hours  Financial Stress  \\\n",
      "0                                   Yes               3.0               1.0   \n",
      "1                                    No               3.0               2.0   \n",
      "2                                    No               9.0               1.0   \n",
      "3                                   Yes               4.0               5.0   \n",
      "4                                   Yes               1.0               1.0   \n",
      "\n",
      "  Family History of Mental Illness  Depression  \n",
      "0                               No         1.0  \n",
      "1                              Yes         0.0  \n",
      "2                              Yes         0.0  \n",
      "3                              Yes         1.0  \n",
      "4                               No         0.0  \n"
     ]
    }
   ],
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "\n",
    "with open(r\"C:\\Users\\param\\Downloads\\depression_dataset.arff\", 'r') as f:\n",
    "    dataset = arff.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(dataset['data'])\n",
    "df.columns = [attr[0] for attr in dataset['attributes']]\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d0c84-d7d9-4d76-8f24-5a3eb7ec4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
